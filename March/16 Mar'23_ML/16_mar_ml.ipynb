{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "Overfitting: Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations that are not representative of the underlying patterns. Consequences include poor generalization to new, unseen data, and the model may perform well on the training set but poorly on real-world examples. To mitigate overfitting, techniques like regularization, reducing model complexity, and increasing the amount of training data can be applied.\n",
    "\n",
    "Underfitting: Underfitting happens when a model is too simple to capture the underlying patterns in the training data. Consequences include poor performance on both the training and test data, as the model fails to learn the underlying structure. Increasing model complexity, adding more features, or using a more sophisticated algorithm can help mitigate underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "To reduce overfitting, you can:\n",
    "\n",
    "Use regularization techniques (e.g., L1 or L2 regularization).\n",
    "Decrease model complexity.\n",
    "Increase the amount of training data.\n",
    "Apply dropout in neural networks.\n",
    "Use cross-validation to assess model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns. Scenarios where underfitting can occur include:\n",
    "\n",
    "Using a linear model for a non-linear problem.\n",
    "Insufficient training data.\n",
    "Too aggressive regularization.\n",
    "Choosing a model that is inherently too simple for the task.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "\n",
    "The bias-variance tradeoff represents the balance between bias (error due to overly simplistic assumptions) and variance (error due to too much complexity). High bias models tend to underfit, while high variance models tend to overfit. Finding the right balance is crucial for optimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "Common methods for detecting overfitting and underfitting include:\n",
    "\n",
    "Monitoring training and validation performance during training.\n",
    "Using cross-validation to assess model performance on different subsets of the data.\n",
    "Analyzing learning curves to identify patterns of overfitting or underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "High bias (underfitting) models include simple models that do not capture the underlying complexity of the data.\n",
    "High variance (overfitting) models include complex models that fit the training data too closely but fail to generalize to new data.\n",
    "High bias models have poor performance on both training and test data, while high variance models have good performance on the training data but poor generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the model's cost function. Common regularization techniques include:\n",
    "\n",
    "L1 Regularization (Lasso): Adds the absolute values of the coefficients as a penalty term.\n",
    "L2 Regularization (Ridge): Adds the squared values of the coefficients as a penalty term.\n",
    "Elastic Net: Combines both L1 and L2 regularization.\n",
    "Dropout: Randomly drops neurons during training in neural networks, preventing over-reliance on specific neurons."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
